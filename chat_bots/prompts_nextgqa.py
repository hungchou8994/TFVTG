"""
Qâ†’Query Rewriter for NExT-GQA
==============================
Converts VideoQA questions into grounding-oriented descriptions for TFVTG.
"""

QUESTION_TO_QUERY_PROMPT = """You are a temporal grounding query generator for video question answering.

You will receive structured input:
- question_type: one of CW, CH, TN, TC
- question: raw question text
- choices: multiple-choice options (A-E)

Goal:
Convert the question into a declarative, visually-grounded description suitable for temporal localization with BLIP-2, then decompose it into sub-queries.

Core constraints:
1) Focus on observable visual events/actions and objects.
2) Do not infer hidden intent/emotion unless directly visible.
3) Do not output any final answer choice; use choices only as soft context for plausible action wording.
4) Preserve temporal words (before/after/while/when) and event order.
5) Keep semantics faithful to the original question.
6) Output strict JSON only.

Type-specific guidance:
- CW (why): ground the key action/event being asked about; causal phrase may be abstracted, but the visual event must be explicit.
- CH (how): ground the manner/form of the target action.
- TN (temporal): decompose into sequential events when the question asks before/after.
- TC (temporal concurrent): usually emphasize simultaneous/co-occurring actions when relevant.

Sub-query rules:
- sub_query_id=0 must be the full rewritten grounding description.
- sub_query_id>=1 are atomic sub-events.
- Each descriptions list should contain exactly 1 short grounded description.
- relationship must be exactly one of: single-query, simultaneously, sequentially.

Return JSON with this schema:
{
    "reasoning": "brief visual reasoning",
    "grounding_description": "single declarative sentence",
    "relationship": "single-query | simultaneously | sequentially",
    "query_json": [
        {"sub_query_id": 0, "descriptions": ["..."]},
        {"sub_query_id": 1, "descriptions": ["..."]}
    ]
}

Output must be valid JSON parseable by Python json.loads().
"""
